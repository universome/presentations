\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{indentfirst}
\usepackage{bm}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{graphicx}
\usepackage{tensor}
\usepackage{biblatex}
\usepackage{titlesec}
\usepackage{mathtools} % for \coloneqq
\usepackage[bottom]{footmisc} % prevents pictures going below footnote :|

\DeclareMathOperator{\supp}{supp}

\title{Theoretical assignment 4; 15 points total + 3 points extra}
\author{Theoretical Deep Learning course, MIPT}
\date{}

\begin{document}

\maketitle

Here by capital letters we denote random variables, and by small letters --- their values.
That is, $x \in \supp{X}$ is a value of random variable $X$ with support $\supp{X}$.

\section*{Problem 1}

\textbf{5 points total.}

\begin{enumerate}
    \item \textbf{1 point.} Prove that Kullback-Leibler divergence is non-negative.
    \item \textbf{0.5 points.} Prove that mutual information is non-negative.
    \item \textbf{0.5 points.} Prove that conditioning reduces entropy: $H(X | Y) \leq H(X)$ (this holds for both entropy and differential entropy).
    \item \textbf{2 points.} Is it true, that conditioning reduces mutual information: $I(X; Y | Z) \leq I(X; Y)$? Or maybe conditioning increases it $I(X;Y|Z) \geq I(X;Y)$?
    \item \textbf{1 point.} Prove that $I(X; f(Y)) \leq I(X; Y)$ for any deterministic function $f$.
\end{enumerate}

\section*{Problem 2}

\textbf{1 point.}

Consider a Markov chain $X_1 \to X_2 \to \ldots \to X_n$.
Prove that a sequence of random variables $X_n, \ldots, X_1$ is also a Markov chain.

\section*{Problem 3}

\textbf{1 point.}

Let $\mathcal{Q}$ be a set of all factorized density functions, i.e.
\[
%q(\bm x) \in \mathcal{Q} \Longleftrightarrow q(\bm x) = \prod_{i=1}^n q(x_i)
\mathcal{Q} = \big\{ q(\bm x)\ \big|\ q(\bm x) = \prod_{i=1}^n q(x_i) \big\}
\]

Consider some density $p(\bm x)$ (not necessarily from $\mathcal{Q}$).
Prove that
\[
    q_*(\bm x) = \arg \min_{q \in \mathcal{Q}} \text{KL}[p(\bm x) \| q(\bm x)] \Longleftrightarrow q_*(\bm x) = \prod_{i=1}^n p(x_i)
\]

Hence if we want to approximate some distribution $p(\bm x)$ with a factorized one, we should better take the product of its marginals.

\section*{Problem 4}

\textbf{2 points.}

Consider a Markov chain $X_1 \to X_2 \to ... \to X_n$.
Prove that
\[
    I(X_1; X_n) \leq \min_{k<n} I(X_k, X_{k+1})
\]

This means that amount of information passed along the chain can't be larger than the ``bandwidth'' of its tightest link.

\section*{Problem 5}

\textbf{6 points total + 3 points extra.}

Consider a Markov chain $Y \to X \to Z$, where $Z = f(X)$ for a \textit{deterministic} neural network $f$.
We are going to prove that in this case $I(X;Z)$ is either infinite (if $X$ is continuous) or constant (if $X$ is discrete) regardless of training process.
Proofs will be completely different for these two cases.

\begin{enumerate}
    \item \textbf{Continuous case.}
    
    Let $X$ be a continuous random variable.
    \begin{enumerate}
        \item \textbf{1 point.} 
        Prove that if $f$ is injective, we have:
        \[
            I(X; f(X)) = \infty.
        \]
        
        \item \textbf{2 points.} 
        Prove that the same holds for $f$ such that a set $f^{-1}(x)$ is finite for every $x \in \supp{X}$.
        
        \item \textbf{3 points extra.}
        Prove that the same holds if $f(X)$ has support of positive Lebesgue measure.
        The proof of point 1 of Theorem 2.4 in \url{http://people.lids.mit.edu/yp/homepage/data/itlectures_v5.pdf} might help you.
        
    \end{enumerate}
        
    \item \textbf{Discrete case.}
    
    \begin{enumerate}
        \item \textbf{2 points.} 
        Let $X$ be some discrete random variable with a finite or countable support $S_X = \supp{X}$ and $f_\theta(x)$ be a neural network with any injective non-linearity $\sigma$ (sigmoid, tanh, LeakyReLU, etc):
        \[
        f_\theta(x) = \sigma(W_k(...(W_2( W_1(x) + b_1) + b_2)...) + b_k) \qquad \theta = \{W_1, ..., W_k, b_1, ..., b_k \}
        \]
        Prove that the set of weights $\tilde{\Theta}$ for which $f_\theta(x)$ is not injective on $S_X$:
        \[
        \tilde{\Theta} = \{ \theta \ |\ \exists\ x_i, x_j \in S_X, x_i \neq x_j, f_\theta(x_i) = f_\theta(x_j) \}
        \]
        is a measure zero set.
        This means that if we have a discrete distribution then our output (and all intermediate activations) are different for different inputs almost surely.
    
        \item \textbf{1 point.} 
        Let $X$ be some discrete random variable. Prove that there is some $c \in \mathbb{R}$ such that for any function $f$ which is injective for $X$ (i.e. it's injective on $\supp{X}$) we have $I(X; f(X)) = c$.
    \end{enumerate}

\end{enumerate}

\end{document}
