\documentclass[10pt]{beamer}

%\usepackage[backend=bibtex,firstinits=true,style=verbose-inote,citestyle=authortitle]{biblatex}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{filecontents}
\usepackage{biblatex}
\input{../new-commands.tex}

%\usecolortheme{dolphin}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
\begin{filecontents*}{references.bib}
@inproceedings{
classifier_ebm,
title={Your classifier is secretly an energy based model and you should treat it like one},
author={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkxzx0NtDB}
}
\end{filecontents*}
\addbibresource{references.bib}

\title{Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One\footnote{Will Grathwohl et al., ICLR 2020}}
%\subtitle{}
%\author{Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky}
%\date{}
%\logo{\includegraphics[height=1cm]{images/ipavlov-logo.png}}

\newcommand{\citepaper}[1]{\citetitle{#1} by \citeauthor{#1}}

%\graphicspath{{./images}}

%\usetheme{lucid}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{What is an Energy Based Model (EBM)?}
\pause
Any pdf $p_\theta(x)$ can be written down as:
    
    \begin{equation}
p_{\theta}(\mathbf{x})=\frac{\exp \left(-E_{\theta}(\mathbf{x})\right)}{Z_\theta}
\end{equation}
\pause
where $Z_\theta$ is a normalizing constant:
\begin{equation*}
Z_\theta = \int_{\bm x} \exp{(-E_\theta(\bm x))}d\bm x
\end{equation*}

\begin{itemize}
    \item\pause This parametrization allows us to forget about $p_\theta(x)$ and work with $E_\theta(x)$
    \item\pause Function $E_\theta(x)$ is an \textit{energy function} (hence the name --- EBM)
    \item\pause Energy functions are \textit{easier} to work with: you do not have any restrictions on $E_\theta(x)$ ($p_\theta(x)$ is nonnegative and integrates to 1).
    \item\pause Energy functions are \textit{harder} to work with: $Z_\theta$ is hard or impossible to compute, so we can't optimize EBM for $\theta$
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How to train EBMs}
    
    \begin{itemize}
        \item\pause We perform standard MLE: maximize $\log p_\theta(x_i)$ at our points $x_i$.
        \item\pause By optimizing $\log p_\theta(x)$ we also optimize $E_\theta(x)$.
        \item\pause We can't compute $\log p_\theta(x)$, but we can (approximately) compute its log-derivative via:
\begin{equation}
\frac{\partial \log p_{\theta}(\mathbf{x})}{\partial \theta}=\mathbb{E}_{p_{\theta}\left(\mathbf{x}^{\prime}\right)}\left[\frac{\partial E_{\theta}\left(\mathbf{x}^{\prime}\right)}{\partial \theta}\right]-\frac{\partial E_{\theta}(\mathbf{x})}{\partial \theta}
\end{equation}
        \item\pause Proof: follows directly from the definition.
        \item\pause But a new problem arises: how to compute expectation?
        \item\pause Answer: we can't do that directly, so let's use SGLD sampling.
    \end{itemize}
\end{frame}

\begin{frame}    
    \frametitle{Stochastic Gradient Langevin Dynamics (SGLD)}
    Motivation:
    \begin{itemize}
        \item\pause Imagine we have a ``magic'' function $p_\theta(x)$ that takes an input $x$ and tells us its density.
        \item\pause Unfortunately, we cannot sample new $x$-s from it directly, i.e. $p_\theta(x)$ does not support such operations.
        \item\pause But we really want to sample from it! What should we do?
        \item\pause SGLD is a method to sample from such functions.
        \item\pause What is cool about SGLD is that it allows to use \textit{unnormalized} density $\tilde{p}(x)$ (a very important property in our case!).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Stochastic Gradient Langevin Dynamics (SGLD)}
    SGLD in a (very) few words:
    \begin{enumerate}
        \item\pause Start from any random point $x_0$.
        \item\pause Compute the gradient of $\nabla \log p_\theta(x_0)$.
        \item\pause Perform a small step in that direction.
        \item\pause Add a little bit of noise.
        \item\pause Obtain $x_{t+1}$. Accept it if:
        \begin{equation}
\frac{p\left(x_{t} | x_{t+1}\right) p\left(x_{t}\right)}{p\left(x_{t+1} | x_{t}\right) p\left(x_{t+1}\right)}<u, u \sim \mathcal{U}[0,1]
\end{equation}
    \end{enumerate}
    \pause In other words, run the following recurrence a lot of times:
\begin{equation}
\mathbf{x}_{0} \sim p_{0}(\mathbf{x}), \quad \mathbf{x}_{i+1}=\mathbf{x}_{i}-\frac{\alpha}{2} \frac{\partial E_{\theta}\left(\mathbf{x}_{i}\right)}{\partial \mathbf{x}_{i}}+\epsilon, \quad \epsilon \sim \mathcal{N}(0, \alpha)
\end{equation}

\pause Justification: if your model and your choice of $\alpha$ are ``good'' (in some sense), then after infinite amount of steps your samples will be ``good'' (in some sense).

\end{frame}

\begin{frame}
    \frametitle{Ok, how to treat a classifier as an EBM?}
    
    \begin{itemize}
        \item\pause Remember that \textit{any} function can be used as $E_\theta(x)$
        \item\pause Let $f_\theta(x)$ be our neural network.
        \item\pause Let $f_\theta(x)[y]$ be the $y$-th logit (i.e. logit value for class $y$).
        \item\pause Let our examples be $s = (x,y)$
        \item\pause We define energy $E_\theta(s) = E_\theta(x,y) = -f_\theta(x)[y]$.
        \item\pause Using $E_\theta(x,y)$ we can define $p_\theta(x)$:
        \begin{equation}
p_{\theta}(\mathbf{x})=\sum_{y} p_{\theta}(\mathbf{x}, y)=\frac{1}{Z_\theta}\sum_{y} \exp \left(f_{\theta}(\mathbf{x})[y]\right)
\end{equation}
        \item\pause Using $p_\theta(x)$ we can define $E_\theta(x)$:
        \begin{equation}
E_{\theta}(\mathbf{x})=-\log \sum_{y} \exp \left(f_{\theta}(\mathbf{x})[y]\right)
\end{equation}
        \item\pause Higher the logits --- lower the energy of $x$.
        \item\pause Our final loss looks like:
        \begin{equation}
            \mathcal{L}(x) = \log p(x,y) = \underbrace{\log p(y|x)}_\text{classification loss} + \underbrace{\log p(x)}_\text{generative loss}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Is JEM using adversarial training?!}
    \begin{itemize}
        \item\pause Let's take a closer look to the gradient of the part of the loss:
\begin{equation}
\frac{\partial \log p_{\theta}(\mathbf{x})}{\partial \theta}=\mathbb{E}_{p_{\theta}\left(\mathbf{x}^{\prime}\right)}\left[\frac{\partial E_{\theta}\left(\mathbf{x}^{\prime}\right)}{\partial \theta}\right]-\frac{\partial E_{\theta}(\mathbf{x})}{\partial \theta}
\end{equation}
    \item\pause If we use a single sample to approximate expectation and compute the gradient across the dataset our gradient we'll look like:
\begin{equation}
\frac{\partial \log p_{\theta}(\mathbf{X})}{\partial \theta} \approx \frac{1}{n}\sum_{i=1}^n\left(\frac{\partial E_{\theta}\left(\mathbf{x}^\text{fake}\right)}{\partial \theta}-\frac{\partial E_{\theta}(\mathbf{x}^\text{real})}{\partial \theta}\right)
\end{equation}
    \item\pause And this gradient looks very similar to the gradient of the following loss:
    \begin{equation}
        \mathcal{L}(\theta) = \expect[p_\theta(x)]{E_\theta(x)} - \expect[p_\text{data}(x)]{E_\theta(x)}
    \end{equation}
    \item\pause Which is a discriminator loss!
    \item\pause Reformulation of GANs via EBMs is not new.
    \item\pause The key novelty of the paper is that we can do that ``inside'' the classifier.
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results and discussion}
    \begin{itemize}
        \item\pause Disclaimer: results are really good.
        \item\pause Authors built a model on SVHN/CIFAR10/CIFAR100 which:
        \begin{itemize}
            \item\pause has good classification accuracy
            \item\pause has good IS/FID scores (i.e. has good samples)
            \item\pause has much better calibrated predictions (i.e. is not overconfident unlike traditional classifiers)
            \item\pause has good out-of-distribution predictions (i.e. detecting when we feed OOD-samples into it)
            \item\pause is robust to adversarial attacks
            \item\pause ...and all of these at the same time!
        \end{itemize}
        \item\pause A huge limitation: SGLD makes both the training unstable and the model hard to scale.
        %\item\pause A big limitation: we cannot compute the loss value!!!
        \item\pause An approach was coined Joint Energy based Model (JEM) since we model $(x,y)$ simultaneously.
    \end{itemize}
\end{frame}

\end{document}
